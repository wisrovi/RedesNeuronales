{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nuevo editor de código de Colab",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wisrovi/RedesNeuronales/blob/master/AprendizajePorRefuerzo-PongDeterministic2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDELHJVDYxoA",
        "colab_type": "text"
      },
      "source": [
        "# Teoría"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMDUzanRLoUS",
        "colab_type": "text"
      },
      "source": [
        "## Objetivo: \n",
        "Repasar los conceptos vistos en clase.\n",
        "\n",
        "La puntuación de este bloque es de 4 puntos sobre la nota final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMo0eBLY1Gs",
        "colab_type": "text"
      },
      "source": [
        "**Define brevemente qué es el aprendizaje por refuerzo. ¿Qué diferencias hay entre aprendizaje supervisado, no supervisado y por refuerzo?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izoGROmUY1OO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lug2H4ygZCGU",
        "colab_type": "text"
      },
      "source": [
        "**Define con tus palabras los conceptos de Entorno, Agente, Recompensa, Estado y Observación.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd5veW0eZCO7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6GMFSLZCLw",
        "colab_type": "text"
      },
      "source": [
        "**Dependiendo del algoritmo de aprendizaje por refuerzo que se use, ¿qué clasificaciones podemos encontrar? Coméntalas brevemente.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cW1RBOpZCJ-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw-6C3J3Y1MJ",
        "colab_type": "text"
      },
      "source": [
        "**Lista tres diferencias entre los algoritmos de DQN y Policy Gradient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22_VeKRfZYXN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98NdMWLxaQv-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FwICPIVaQ8N",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iTYuIoLawAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_zXk0PXZYad",
        "colab_type": "text"
      },
      "source": [
        "# Práctica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkgoJNjdZ6U2",
        "colab_type": "text"
      },
      "source": [
        "Objetivo: Implementar una solución, usando keras-rl y basada en el algoritmo de DQN visto en clase, para que un agente aprenda una estrategia ganadora en el juego del Pong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI0finZIZ6R9",
        "colab_type": "text"
      },
      "source": [
        "La puntuación de este bloque es de 6 puntos sobre la nota final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXpPfxW3Y1Jw",
        "colab_type": "text"
      },
      "source": [
        "El entorno sobre el que trabajaremos será _PongDeterministic-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "Para evaluar cómo lo está haciendo el agente, la recompensa en el _Pong_ oscila, aproximadamente, en el rango de valores **[-20, 20]**. La estrategia óptima de un agente estaría alrededor de una media de recompensa de 20.\n",
        "\n",
        "- **NOTA IMPORTANTE**: Si el agente no llegara a aprender una estrategia ganadora, responder sobre la mejor estrategia obtenida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOYtKZSVbMME",
        "colab_type": "text"
      },
      "source": [
        "# Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK7RPnuKQpgR",
        "colab_type": "code",
        "outputId": "5cfea3a7-f00c-4eca-e7c5-3dee46bdd504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "#https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0\n",
        "from IPython.display import display, HTML\n",
        "js = ('<script>function ConnectButton(){ '\n",
        "        'console.log(\"Connect pushed\"); '\n",
        "        'document.querySelector(\"#connect\").click()} '\n",
        "        'setInterval(ConnectButton,3000);</script>')\n",
        "display(HTML(js))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script>function ConnectButton(){ console.log(\"Connect pushed\"); document.querySelector(\"#connect\").click()} setInterval(ConnectButton,3000);</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2KQ1oCCdwgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "isColabGoogle = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qN7SmbMbN40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if isColabGoogle:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    BASE_FOLDER = '/content/gdrive/My Drive/Master IA/AprendizajePorRefuerzo/'\n",
        "else:\n",
        "    BASE_FOLDER = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5L2VhyVEXlh",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "d421d099-c21f-41b0-902e-fe2e93f024a4"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6f391e0a-c919-49ad-a0b8-e46dfff470d5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6f391e0a-c919-49ad-a0b8-e46dfff470d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dqn_PongDeterministic-v0_weights2.h5f to dqn_PongDeterministic-v0_weights2.h5f\n",
            "User uploaded file \"dqn_PongDeterministic-v0_weights2.h5f\" with length 1193384 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNE_e7W04nJ2",
        "colab_type": "code",
        "outputId": "bf6687dc-04e8-46b7-cde9-f77846b70e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls '/content/gdrive/My Drive/Master IA/AprendizajePorRefuerzo/'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/gdrive/My Drive/Master IA/AprendizajePorRefuerzo/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Jv4Riqa1AZ",
        "colab_type": "text"
      },
      "source": [
        "# Install and import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLBeG9qiRRYK",
        "colab_type": "text"
      },
      "source": [
        "### Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPRbpTdwLE8T",
        "colab_type": "code",
        "outputId": "fad8d8a8-6d81-4555-a1af-7b381cb6366d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install keras-rl==0.4.2\n",
        "\n",
        "#!pip install tensorflow==1.13.1\n",
        "!pip install tensorflow-gpu==1.14\n",
        "\n",
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "!pip install h5py\n",
        "\n",
        "\n",
        "#creo son necesarias, pero no se ha comprobado que en su ausencia falle\n",
        "!pip install jupyter\n",
        "!pip install torch"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl==0.4.2) (2.2.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.3.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.17.4)\n",
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.17.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (3.10.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 41.2MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 24.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.1.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14) (41.6.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow-gpu 1.4.0\n",
            "    Uninstalling tensorflow-gpu-1.4.0:\n",
            "      Successfully uninstalled tensorflow-gpu-1.4.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym) (3.4.7.28)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.4.7.28)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari]) (0.46)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.17.4)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter) (4.6.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter) (4.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.2.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter) (7.5.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter) (2.1.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter) (4.6.1)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter) (5.3.4)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter) (4.3.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.4.4)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (4.4.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (1.4.2)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (2.10.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.8.4)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (4.5.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (0.8.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter) (1.0.18)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter) (3.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter) (2.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter) (17.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets->qtconsole->jupyter) (1.12.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->qtconsole->jupyter) (4.4.1)\n",
            "Requirement already satisfied: html5lib!=0.9999,!=0.99999,<0.99999999,>=0.999 in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter) (0.9999999)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter) (0.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter) (4.7.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter) (41.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter) (0.1.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBf1tNuRRWkd",
        "colab_type": "text"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbyoXvaEYwuT",
        "colab_type": "code",
        "outputId": "ecaecb8b-5ca7-47ff-88ad-a27722731c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Para las librerias para la red neuronal\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "from keras.callbacks import TensorBoard\n",
        "import os "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkexVSv4RbnT",
        "colab_type": "text"
      },
      "source": [
        "### Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JwqwV1RUO6x",
        "colab_type": "code",
        "outputId": "d9121ecf-c08a-4627-f788-d0f2c46e1ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ENV_NAME = 'PongDeterministic-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym.make(ENV_NAME)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# random seed\n",
        "np.random.seed(123)\n",
        "env.seed(123)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[123, 151010689]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWfdcMqFUO-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the input shape to resize the screen\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "# This processor will be similar to the Atari processor\n",
        "class PongProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        \n",
        "        img = Image.fromarray(observation)\n",
        "        # resize and convert to grayscale\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        \n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6QI3zYOLOwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor = PongProcessor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfGxs5qUaxo0",
        "colab_type": "text"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6GlW1ZUUO8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createModel(transferModel = None):\n",
        "    input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "\n",
        "    model = Sequential()\n",
        "    if transferModel is not None:\n",
        "        model.add(transferModel)\n",
        "    else:\n",
        "        model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "    \n",
        "    model.add( Convolution2D(32, kernel_size=(8, 8), strides=(4, 4), activation='relu', padding='same'  ) )\n",
        "    model.add( Convolution2D(64, kernel_size=(4, 4), strides=(2, 2), activation='relu', padding='same'  ) )\n",
        "    model.add(BatchNormalization())    \n",
        "    model.add( Convolution2D(128, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same'  ) ) \n",
        "    model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "    #BatchNormalization \n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(nb_actions, activation='linear'))   #model.add(Dense(nb_actions, activation='sigmoid'))    \n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc9Tqn5sLT1z",
        "colab_type": "code",
        "outputId": "1a6e5074-6f5e-4116-821c-bf3615407f36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        }
      },
      "source": [
        "model = createModel()\n",
        "print(model.summary())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 21, 21, 32)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        32832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6, 6, 512)         66048     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 110598    \n",
            "=================================================================\n",
            "Total params: 291,814\n",
            "Trainable params: 291,686\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jjRvhJ6a7rv",
        "colab_type": "text"
      },
      "source": [
        "# Memory and policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni1mhPqvUO32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
        "                              nb_steps=1000000)\n",
        "\n",
        "#policy = BoltzmannQPolicy(tau=1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKjMsMxaa_zw",
        "colab_type": "text"
      },
      "source": [
        "# Agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz6gy7ZKbCzg",
        "colab_type": "text"
      },
      "source": [
        "## Create"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhrHpy3UUVSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn = DQNAgent(\n",
        "    nb_actions=nb_actions, \n",
        "    model=model, \n",
        "    policy=policy, \n",
        "    memory=memory,\n",
        "    processor=processor, \n",
        "    nb_steps_warmup=10000, \n",
        "    target_model_update=1e-2,\n",
        "    gamma=.99)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkGwB8CfeWOn",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27SQvAKFeYUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(lr=.00025)\n",
        "optimizer = SGD()\n",
        "optimizer = RMSprop (lr = .00025, rho = 0.95, epsilon = 0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gev9xBjbGQT",
        "colab_type": "text"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_IJhPfUUVVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3633e6a8-c5a0-439a-9808-51ca2b07bcd6"
      },
      "source": [
        "#compilar modelo\n",
        "dqn.compile(optimizer, metrics=['mae'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msPKZklecYeQ",
        "colab_type": "text"
      },
      "source": [
        "## callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVyUNsCQcdcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg9-i8fhUVb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Para algunos casos es importante saber cual entrenamiento fue mejor, \n",
        "#este callback guarda el modelo tras cada epoca completada con el fin de si luego se desea un registro de pesos para cada epoca\n",
        "#Se ha usado este callback para poder optener el mejor modelo de pesos, sobretodo en la red neuronal creada desde cero\n",
        "#siendo de gran utilidad para determinar el como ir modificando los layer hasta obtener el mejor modelo\n",
        "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "guardarModeloCadaCiertoIntervaloDeSteps = ModelIntervalCheckpoint(checkpoint_weights_filename, interval=50000)\n",
        "\n",
        "callbacks.append(guardarModeloCadaCiertoIntervaloDeSteps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udkGJRdCcgvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_filename = 'dqn_{}_log.json'.format(ENV_NAME)\n",
        "guardarModelo = FileLogger(log_filename, interval=100)\n",
        "\n",
        "callbacks.append(guardarModelo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyORz4SFFEm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tb_log_dir = 'logs/tmp'\n",
        "tb_callback = TensorBoard(log_dir=tb_log_dir, histogram_freq=0, write_graph=True)\n",
        "\n",
        "callbacks.append(tb_callback)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IgkOgT63aVs",
        "colab_type": "code",
        "outputId": "8712f769-d7f1-40dc-ada7-66ffd144f249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "#EarlyStopping, detener el entrenamiento una vez que su pérdida comienza a aumentar \n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    patience=5, #argumento de patience representa el número de épocas antes de detenerse una vez que su pérdida comienza a aumentar (deja de mejorar). \n",
        "    min_delta=0,  #es un umbral para cuantificar una pérdida en alguna época como mejora o no. Si la diferencia de pérdida es inferior a min_delta , se cuantifica como no mejora. Es mejor dejarlo como 0 ya que estamos interesados ​​en cuando la pérdida empeora.\n",
        "    mode='auto')\n",
        "\n",
        "callbacks.append(early_stop)\n",
        "\"\"\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#EarlyStopping, detener el entrenamiento una vez que su pérdida comienza a aumentar \\nearly_stop = EarlyStopping(\\n    monitor='val_loss', \\n    patience=5, #argumento de patience representa el número de épocas antes de detenerse una vez que su pérdida comienza a aumentar (deja de mejorar). \\n    min_delta=0,  #es un umbral para cuantificar una pérdida en alguna época como mejora o no. Si la diferencia de pérdida es inferior a min_delta , se cuantifica como no mejora. Es mejor dejarlo como 0 ya que estamos interesados \\u200b\\u200ben cuando la pérdida empeora.\\n    mode='auto')\\n\\ncallbacks.append(early_stop)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YknGtovJ3lVp",
        "colab_type": "code",
        "outputId": "250f011b-2591-4489-c042-9e49916ece86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "#ReduceLROnPlateau, que si el entrenamiento no mejora tras unos epochs específicos, reduce el valor de learning rate del modelo\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss', \n",
        "    factor=0.1, \n",
        "    patience=5, \n",
        "    epsilon=1e-4, \n",
        "    mode='min')\n",
        "\n",
        "callbacks.append(reduce_lr)\n",
        "\"\"\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#ReduceLROnPlateau, que si el entrenamiento no mejora tras unos epochs específicos, reduce el valor de learning rate del modelo\\nreduce_lr = ReduceLROnPlateau(\\n    monitor='val_loss', \\n    factor=0.1, \\n    patience=5, \\n    epsilon=1e-4, \\n    mode='min')\\n\\ncallbacks.append(reduce_lr)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx7bjsbqcmR_",
        "colab_type": "code",
        "outputId": "f7797b20-542b-4f48-b39d-bd21b3c5376c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(callbacks)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtiWUrHcbTVL",
        "colab_type": "text"
      },
      "source": [
        "# Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU1UN8Qbe9CN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entrenar = True\n",
        "usarPesosEntrenados = True\n",
        "testearAlFinalEntrenamiento = False  #si se hace un testeo, se guarda solo si hay mejoria\n",
        "guardarSiempreDespuesCadaEntrenamiento = True\n",
        "graficosVisualesGame = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmMWtVozfM9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "totalSteps = 5000000\n",
        "intervalo = 5000\n",
        "\n",
        "cantidadEntrenamientos = 100 #cantidad de veces que repito el entrenamiento con el totalSteps\n",
        "\n",
        "#si totalSteps = 100000 y intervalo = 10000, entonces se haran 10 intervalos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G916NzbUgYiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#archivo guardar pesos\n",
        "weights_filename = 'dqn_' + ENV_NAME + '_weights2.h5f'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9V3ZUZ52f0P",
        "colab_type": "code",
        "outputId": "23899c72-915e-478f-e565-e0e75e6bc8ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "if usarPesosEntrenados:    \n",
        "    if isColabGoogle:\n",
        "        print(\"Archivos disponibles: \")\n",
        "        !ls '/content/gdrive/My Drive/Master IA/AprendizajePorRefuerzo/'\n",
        "        print(\"Buscando archivo: \" + weights_filename)\n",
        "    try:        \n",
        "        dqn.load_weights(BASE_FOLDER + weights_filename)\n",
        "        scoreOld = dqn.test(env, nb_episodes=5, visualize=graficosVisualesGame).history['episode_reward'][1]\n",
        "        print(scoreOld)\n",
        "    except:\n",
        "        scoreOld = -50\n",
        "        print(\"No se encontro archivos de pesos\")\n",
        "        pass"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -14.000, steps: 3018\n",
            "Episode 2: reward: -14.000, steps: 2721\n",
            "Episode 3: reward: -16.000, steps: 2264\n",
            "Episode 4: reward: -14.000, steps: 2399\n",
            "Episode 5: reward: -12.000, steps: 2675\n",
            "-14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfd2Asw22h-r",
        "colab_type": "code",
        "outputId": "3e69b932-f612-4b11-8ade-b89f21f42310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "if entrenar:    \n",
        "    try:\n",
        "        dqn.fit(env, \n",
        "                callbacks=callbacks, \n",
        "                nb_steps=31000, \n",
        "                log_interval=5000, \n",
        "                visualize=graficosVisualesGame) #, \n",
        "                #verbose=2)\n",
        "    except:\n",
        "        pass\n",
        "    dqn.test(env, nb_episodes=5, visualize=graficosVisualesGame)\n",
        "    dqn.save_weights(BASE_FOLDER + weights_filename, overwrite=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Training for 31000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 34s 3ms/step - reward: -0.0204\n",
            "10 episodes - episode_reward: -20.000 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -0.0215\n",
            "11 episodes - episode_reward: -19.909 [-21.000, -19.000] - loss: 0.016 - mean_absolute_error: 7.440 - mean_q: 9.051 - mean_eps: 0.986 - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -0.0200\n",
            "10 episodes - episode_reward: -19.900 [-21.000, -18.000] - loss: 0.018 - mean_absolute_error: 8.174 - mean_q: 9.940 - mean_eps: 0.978 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 1000/10000 [==>...........................] - ETA: 3:21 - reward: -0.0230done, took 499.730 seconds\n",
            "Testing for 3 episodes ...\n",
            "Episode 1: reward: -15.000, steps: 2056\n",
            "Episode 2: reward: -13.000, steps: 2156\n",
            "Episode 3: reward: -15.000, steps: 1809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJVHCGUMUVe2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0967f7d0-563c-420c-eb6c-21c4aa80b506"
      },
      "source": [
        "if entrenar:\n",
        "    conteo = 0\n",
        "    entrenar = False\n",
        "    agenteOld = None\n",
        "    for i in range(cantidadEntrenamientos):\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"**                                                 **\")\n",
        "        print(\"**                                                 **\")\n",
        "        print(\"**                                                 **\")\n",
        "        print(\"**                       %d                         **\" %(i))\n",
        "        print(\"**                                                 **\")\n",
        "        print(\"**                                                 **\")\n",
        "        print(\"**                                                 **\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        # Training part\n",
        "        try:\n",
        "            dqn.fit(env, \n",
        "                    callbacks=callbacks, \n",
        "                    nb_steps=totalSteps, \n",
        "                    log_interval=intervalo, \n",
        "                    visualize=graficosVisualesGame) #, \n",
        "                    #verbose=2)            \n",
        "        except:\n",
        "            print(\"Error al entrenar, se ha interrumpido el fit\")\n",
        "            pass\n",
        "        \n",
        "        if guardarSiempreDespuesCadaEntrenamiento:\n",
        "            dqn.save_weights(BASE_FOLDER + weights_filename, overwrite=True)\n",
        "       \n",
        "        if testearAlFinalEntrenamiento:\n",
        "            testEval = dqn.test(env, nb_episodes=5, visualize=graficosVisualesGame).history['episode_reward'][1]       \n",
        "\n",
        "            if scoreOld < testEval:\n",
        "                scoreOld = testEval\n",
        "                agenteOld = dqn\n",
        "                dqn.save_weights(BASE_FOLDER + \"Avanced-\" + weights_filename, overwrite=True)\n",
        "                print(\"Guardando pesos\", scoreOld)\n",
        "            else:\n",
        "                print(\"No hubo mejora, no se guarda este modelo.\")\n",
        "                conteo += 1\n",
        "                if conteo > 3:\n",
        "                    pass\n",
        "                    break\n",
        "                dqn = agenteOld   \n",
        "        else:\n",
        "            dqn.save_weights(BASE_FOLDER + weights_filename, overwrite=True)\n",
        "            dqn.test(env, nb_episodes=5, visualize=graficosVisualesGame)\n",
        "else:    \n",
        "    dqn.load_weights(weights_filename)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       0                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0242\n",
            "5 episodes - episode_reward: -21.000 [-21.000, -21.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "6 episodes - episode_reward: -20.833 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -19.000] - loss: 0.022 - mean_absolute_error: 8.379 - mean_q: 10.181 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0210\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -19.000] - loss: 0.020 - mean_absolute_error: 8.240 - mean_q: 10.013 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0226\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.021 - mean_absolute_error: 8.167 - mean_q: 9.927 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4006/5000 [=======================>......] - ETA: 22s - reward: -0.0225Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -16.000, steps: 2530\n",
            "Episode 2: reward: -15.000, steps: 2240\n",
            "Episode 3: reward: -20.000, steps: 1922\n",
            "Episode 4: reward: -18.000, steps: 2395\n",
            "Episode 5: reward: -14.000, steps: 2687\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       1                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0210\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0218\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.022 - mean_absolute_error: 8.067 - mean_q: 9.806 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0210\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -18.000] - loss: 0.021 - mean_absolute_error: 7.905 - mean_q: 9.609 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0222\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.021 - mean_absolute_error: 7.830 - mean_q: 9.520 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0218\n",
            "5 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.020 - mean_absolute_error: 7.755 - mean_q: 9.425 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            "  93/5000 [..............................] - ETA: 1:47 - reward: -0.0108Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -15.000, steps: 2082\n",
            "Episode 2: reward: -12.000, steps: 2402\n",
            "Episode 3: reward: -13.000, steps: 2251\n",
            "Episode 4: reward: -7.000, steps: 3099\n",
            "Episode 5: reward: -18.000, steps: 2164\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       2                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0214\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0226\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0240\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.021 - mean_absolute_error: 7.614 - mean_q: 9.256 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0212\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - loss: 0.022 - mean_absolute_error: 7.653 - mean_q: 9.307 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0230\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -20.000] - loss: 0.021 - mean_absolute_error: 7.622 - mean_q: 9.266 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4062/5000 [=======================>......] - ETA: 20s - reward: -0.0226Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -17.000, steps: 2262\n",
            "Episode 2: reward: -17.000, steps: 2093\n",
            "Episode 3: reward: -15.000, steps: 2384\n",
            "Episode 4: reward: -18.000, steps: 1745\n",
            "Episode 5: reward: -18.000, steps: 2273\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       3                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0214\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.021 - mean_absolute_error: 7.531 - mean_q: 9.151 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0246\n",
            "6 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.020 - mean_absolute_error: 7.395 - mean_q: 8.987 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0238\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.019 - mean_absolute_error: 7.367 - mean_q: 8.953 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "3894/5000 [======================>.......] - ETA: 25s - reward: -0.0239Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -16.000, steps: 1706\n",
            "Episode 2: reward: -17.000, steps: 1837\n",
            "Episode 3: reward: -12.000, steps: 2259\n",
            "Episode 4: reward: -17.000, steps: 1687\n",
            "Episode 5: reward: -17.000, steps: 1906\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       4                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0222\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0210\n",
            "6 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.019 - mean_absolute_error: 7.159 - mean_q: 8.702 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0230\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -19.000] - loss: 0.017 - mean_absolute_error: 7.071 - mean_q: 8.593 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0248\n",
            "6 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.018 - mean_absolute_error: 6.956 - mean_q: 8.455 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4449/5000 [=========================>....] - ETA: 12s - reward: -0.0207Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -12.000, steps: 2356\n",
            "Episode 2: reward: -11.000, steps: 2732\n",
            "Episode 3: reward: -17.000, steps: 1978\n",
            "Episode 4: reward: -20.000, steps: 2054\n",
            "Episode 5: reward: -11.000, steps: 2507\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       5                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0238\n",
            "5 episodes - episode_reward: -21.000 [-21.000, -21.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0210\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -18.000] - loss: 0.017 - mean_absolute_error: 6.584 - mean_q: 8.003 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0206\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.017 - mean_absolute_error: 6.451 - mean_q: 7.845 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0228\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - loss: 0.016 - mean_absolute_error: 6.346 - mean_q: 7.715 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0196\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -18.000] - loss: 0.017 - mean_absolute_error: 6.329 - mean_q: 7.697 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 244/5000 [>.............................] - ETA: 1:46 - reward: -0.0246Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -13.000, steps: 2449\n",
            "Episode 2: reward: -12.000, steps: 2733\n",
            "Episode 3: reward: -12.000, steps: 2715\n",
            "Episode 4: reward: -17.000, steps: 2236\n",
            "Episode 5: reward: -18.000, steps: 1841\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       6                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0222\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0218\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0220\n",
            "6 episodes - episode_reward: -20.000 [-21.000, -18.000] - loss: 0.017 - mean_absolute_error: 6.382 - mean_q: 7.758 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0238\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.017 - mean_absolute_error: 6.345 - mean_q: 7.715 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0228\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.017 - mean_absolute_error: 6.309 - mean_q: 7.671 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4423/5000 [=========================>....] - ETA: 12s - reward: -0.0194Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -17.000, steps: 2609\n",
            "Episode 2: reward: -18.000, steps: 1894\n",
            "Episode 3: reward: -13.000, steps: 2789\n",
            "Episode 4: reward: -17.000, steps: 2991\n",
            "Episode 5: reward: -16.000, steps: 2385\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       7                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0170\n",
            "4 episodes - episode_reward: -18.750 [-20.000, -16.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0206\n",
            "6 episodes - episode_reward: -20.333 [-21.000, -19.000] - loss: 0.016 - mean_absolute_error: 6.123 - mean_q: 7.443 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0218\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.016 - mean_absolute_error: 6.047 - mean_q: 7.352 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0190\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.015 - mean_absolute_error: 5.966 - mean_q: 7.252 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0204\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.015 - mean_absolute_error: 5.876 - mean_q: 7.145 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            "1943/5000 [==========>...................] - ETA: 1:07 - reward: -0.0180Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -17.000, steps: 2447\n",
            "Episode 2: reward: -17.000, steps: 2782\n",
            "Episode 3: reward: -14.000, steps: 2347\n",
            "Episode 4: reward: -14.000, steps: 2767\n",
            "Episode 5: reward: -17.000, steps: 2247\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       8                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0200\n",
            "4 episodes - episode_reward: -20.500 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0224\n",
            "6 episodes - episode_reward: -20.333 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0194\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - loss: 0.015 - mean_absolute_error: 5.720 - mean_q: 6.958 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0234\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.016 - mean_absolute_error: 5.751 - mean_q: 6.995 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0222\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.015 - mean_absolute_error: 5.743 - mean_q: 6.984 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0226\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -17.000] - loss: 0.015 - mean_absolute_error: 5.739 - mean_q: 6.978 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 171/5000 [>.............................] - ETA: 1:48 - reward: -0.0292Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -20.000, steps: 1473\n",
            "Episode 2: reward: -20.000, steps: 2255\n",
            "Episode 3: reward: -15.000, steps: 2286\n",
            "Episode 4: reward: -16.000, steps: 2095\n",
            "Episode 5: reward: -18.000, steps: 2263\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       9                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -19.600 [-21.000, -17.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0212\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.014 - mean_absolute_error: 5.630 - mean_q: 6.845 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0196\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.015 - mean_absolute_error: 5.531 - mean_q: 6.726 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.015 - mean_absolute_error: 5.489 - mean_q: 6.676 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0208\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -17.000] - loss: 0.014 - mean_absolute_error: 5.433 - mean_q: 6.607 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 749/5000 [===>..........................] - ETA: 1:34 - reward: -0.0267Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -19.000, steps: 2113\n",
            "Episode 2: reward: -15.000, steps: 2505\n",
            "Episode 3: reward: -17.000, steps: 2507\n",
            "Episode 4: reward: -17.000, steps: 2231\n",
            "Episode 5: reward: -16.000, steps: 2597\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       10                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0240\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 109s 22ms/step - reward: -0.0228\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -18.000] - loss: 0.014 - mean_absolute_error: 5.393 - mean_q: 6.556 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0220\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -20.000] - loss: 0.014 - mean_absolute_error: 5.302 - mean_q: 6.450 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0206\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 5.263 - mean_q: 6.401 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4042/5000 [=======================>......] - ETA: 21s - reward: -0.0242Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -15.000, steps: 2499\n",
            "Episode 2: reward: -13.000, steps: 2665\n",
            "Episode 3: reward: -14.000, steps: 2418\n",
            "Episode 4: reward: -13.000, steps: 2613\n",
            "Episode 5: reward: -18.000, steps: 2038\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       11                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0206\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 5.167 - mean_q: 6.287 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0192\n",
            "5 episodes - episode_reward: -19.600 [-21.000, -17.000] - loss: 0.014 - mean_absolute_error: 5.210 - mean_q: 6.338 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0192\n",
            "5 episodes - episode_reward: -19.600 [-21.000, -18.000] - loss: 0.013 - mean_absolute_error: 5.149 - mean_q: 6.264 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0198\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -18.000] - loss: 0.013 - mean_absolute_error: 5.106 - mean_q: 6.211 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            "1430/5000 [=======>......................] - ETA: 1:19 - reward: -0.0154Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -18.000, steps: 2235\n",
            "Episode 2: reward: -13.000, steps: 2541\n",
            "Episode 3: reward: -12.000, steps: 2722\n",
            "Episode 4: reward: -13.000, steps: 2386\n",
            "Episode 5: reward: -12.000, steps: 2531\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       12                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0240\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0196\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 5.055 - mean_q: 6.149 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0224\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -18.000] - loss: 0.014 - mean_absolute_error: 5.066 - mean_q: 6.162 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 5.025 - mean_q: 6.112 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -19.000] - loss: 0.014 - mean_absolute_error: 5.014 - mean_q: 6.101 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 239/5000 [>.............................] - ETA: 1:49 - reward: -0.0251Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -18.000, steps: 2139\n",
            "Episode 2: reward: -14.000, steps: 2460\n",
            "Episode 3: reward: -15.000, steps: 2158\n",
            "Episode 4: reward: -12.000, steps: 2662\n",
            "Episode 5: reward: -15.000, steps: 2503\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       13                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0212\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0230\n",
            "6 episodes - episode_reward: -20.333 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.982 - mean_q: 6.061 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.964 - mean_q: 6.040 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0216\n",
            "6 episodes - episode_reward: -19.667 [-21.000, -17.000] - loss: 0.013 - mean_absolute_error: 4.934 - mean_q: 6.004 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4450/5000 [=========================>....] - ETA: 12s - reward: -0.0218Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -16.000, steps: 2295\n",
            "Episode 2: reward: -15.000, steps: 2519\n",
            "Episode 3: reward: -18.000, steps: 1976\n",
            "Episode 4: reward: -18.000, steps: 2181\n",
            "Episode 5: reward: -15.000, steps: 2166\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       14                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0210\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0232\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0216\n",
            "6 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 4.980 - mean_q: 6.056 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0196\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 4.902 - mean_q: 5.963 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0218\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 4.905 - mean_q: 5.968 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4911/5000 [============================>.] - ETA: 2s - reward: -0.0230Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -16.000, steps: 2158\n",
            "Episode 2: reward: -15.000, steps: 2738\n",
            "Episode 3: reward: -16.000, steps: 2553\n",
            "Episode 4: reward: -13.000, steps: 2500\n",
            "Episode 5: reward: -19.000, steps: 2442\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       15                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 4.877 - mean_q: 5.934 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0222\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 4.910 - mean_q: 5.974 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0224\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -19.000] - loss: 0.014 - mean_absolute_error: 4.949 - mean_q: 6.021 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4467/5000 [=========================>....] - ETA: 11s - reward: -0.0215Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -17.000, steps: 2376\n",
            "Episode 2: reward: -16.000, steps: 2197\n",
            "Episode 3: reward: -16.000, steps: 2087\n",
            "Episode 4: reward: -13.000, steps: 2326\n",
            "Episode 5: reward: -15.000, steps: 2270\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       16                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0196\n",
            "4 episodes - episode_reward: -19.750 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0216\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0238\n",
            "6 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.975 - mean_q: 6.053 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0236\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.014 - mean_absolute_error: 4.987 - mean_q: 6.066 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0214\n",
            "6 episodes - episode_reward: -20.333 [-21.000, -19.000] - loss: 0.014 - mean_absolute_error: 4.941 - mean_q: 6.011 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4944/5000 [============================>.] - ETA: 1s - reward: -0.0206Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -11.000, steps: 2774\n",
            "Episode 2: reward: -16.000, steps: 2399\n",
            "Episode 3: reward: -15.000, steps: 2631\n",
            "Episode 4: reward: -16.000, steps: 3155\n",
            "Episode 5: reward: -18.000, steps: 2340\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       17                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0238\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 4.951 - mean_q: 6.023 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 109s 22ms/step - reward: -0.0190\n",
            "5 episodes - episode_reward: -19.600 [-21.000, -18.000] - loss: 0.013 - mean_absolute_error: 4.980 - mean_q: 6.056 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 109s 22ms/step - reward: -0.0214\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.892 - mean_q: 5.947 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.000 [-20.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.833 - mean_q: 5.880 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 276/5000 [>.............................] - ETA: 1:41 - reward: -0.0254Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -11.000, steps: 2662\n",
            "Episode 2: reward: -16.000, steps: 2131\n",
            "Episode 3: reward: -17.000, steps: 2067\n",
            "Episode 4: reward: -14.000, steps: 2225\n",
            "Episode 5: reward: -9.000, steps: 2693\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       18                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0232\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0258\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0216\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.827 - mean_q: 5.871 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 4.796 - mean_q: 5.831 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 109s 22ms/step - reward: -0.0224\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.817 - mean_q: 5.860 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4442/5000 [=========================>....] - ETA: 12s - reward: -0.0182Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -15.000, steps: 2651\n",
            "Episode 2: reward: -16.000, steps: 2593\n",
            "Episode 3: reward: -16.000, steps: 2137\n",
            "Episode 4: reward: -17.000, steps: 2410\n",
            "Episode 5: reward: -14.000, steps: 2568\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       19                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0212\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.803 - mean_q: 5.844 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0226\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.781 - mean_q: 5.816 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0194\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -17.000] - loss: 0.012 - mean_absolute_error: 4.715 - mean_q: 5.736 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 109s 22ms/step - reward: -0.0188\n",
            "4 episodes - episode_reward: -19.500 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 4.677 - mean_q: 5.689 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 979/5000 [====>.........................] - ETA: 1:27 - reward: -0.0235Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -14.000, steps: 2360\n",
            "Episode 2: reward: -14.000, steps: 2283\n",
            "Episode 3: reward: -16.000, steps: 2309\n",
            "Episode 4: reward: -15.000, steps: 2341\n",
            "Episode 5: reward: -12.000, steps: 2211\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       20                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0182\n",
            "4 episodes - episode_reward: -19.500 [-21.000, -17.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0194\n",
            "5 episodes - episode_reward: -18.800 [-20.000, -17.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0224\n",
            "6 episodes - episode_reward: -20.333 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 4.651 - mean_q: 5.656 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 109s 22ms/step - reward: -0.0212\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 4.590 - mean_q: 5.583 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 109s 22ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 4.599 - mean_q: 5.595 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0210\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 4.634 - mean_q: 5.638 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            "1084/5000 [=====>........................] - ETA: 1:26 - reward: -0.0194Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -13.000, steps: 2879\n",
            "Episode 2: reward: -15.000, steps: 2357\n",
            "Episode 3: reward: -12.000, steps: 3022\n",
            "Episode 4: reward: -13.000, steps: 2960\n",
            "Episode 5: reward: -15.000, steps: 2053\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       21                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0232\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0192\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0234\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -18.000] - loss: 0.012 - mean_absolute_error: 4.579 - mean_q: 5.569 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0230\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 4.516 - mean_q: 5.493 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0204\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -18.000] - loss: 0.012 - mean_absolute_error: 4.502 - mean_q: 5.478 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0188\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -17.000] - loss: 0.012 - mean_absolute_error: 4.587 - mean_q: 5.581 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 454/5000 [=>............................] - ETA: 1:45 - reward: -0.0220Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -14.000, steps: 2639\n",
            "Episode 2: reward: -15.000, steps: 2311\n",
            "Episode 3: reward: -11.000, steps: 2888\n",
            "Episode 4: reward: -12.000, steps: 2825\n",
            "Episode 5: reward: -11.000, steps: 2390\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       22                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0234\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0226\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.606 - mean_q: 5.604 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0218\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 4.662 - mean_q: 5.671 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0226\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.013 - mean_absolute_error: 4.654 - mean_q: 5.663 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4090/5000 [=======================>......] - ETA: 20s - reward: -0.0222Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -11.000, steps: 2828\n",
            "Episode 2: reward: -19.000, steps: 1855\n",
            "Episode 3: reward: -16.000, steps: 2831\n",
            "Episode 4: reward: -18.000, steps: 2547\n",
            "Episode 5: reward: -12.000, steps: 2948\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       23                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0238\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0204\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0204\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 4.624 - mean_q: 5.624 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0198\n",
            "5 episodes - episode_reward: -19.400 [-21.000, -18.000] - loss: 0.012 - mean_absolute_error: 4.548 - mean_q: 5.529 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0232\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 4.447 - mean_q: 5.408 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4973/5000 [============================>.] - ETA: 0s - reward: -0.0213Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -17.000, steps: 2664\n",
            "Episode 2: reward: -17.000, steps: 2550\n",
            "Episode 3: reward: -13.000, steps: 2792\n",
            "Episode 4: reward: -18.000, steps: 2570\n",
            "Episode 5: reward: -18.000, steps: 2310\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       24                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0224\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 4.353 - mean_q: 5.292 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0218\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 4.311 - mean_q: 5.245 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0212\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 4.334 - mean_q: 5.273 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0222\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 4.292 - mean_q: 5.221 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 202/5000 [>.............................] - ETA: 1:48 - reward: -0.0248Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -17.000, steps: 2637\n",
            "Episode 2: reward: -15.000, steps: 2644\n",
            "Episode 3: reward: -17.000, steps: 2771\n",
            "Episode 4: reward: -16.000, steps: 2870\n",
            "Episode 5: reward: -16.000, steps: 2433\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       25                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0226\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0222\n",
            "6 episodes - episode_reward: -20.000 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 4.276 - mean_q: 5.204 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0228\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 4.305 - mean_q: 5.239 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0200\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 4.343 - mean_q: 5.283 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0200\n",
            "5 episodes - episode_reward: -19.400 [-20.000, -19.000] - loss: 0.011 - mean_absolute_error: 4.288 - mean_q: 5.217 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 480/5000 [=>............................] - ETA: 1:40 - reward: -0.0125Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -16.000, steps: 3071\n",
            "Episode 2: reward: -16.000, steps: 2287\n",
            "Episode 3: reward: -14.000, steps: 2737\n",
            "Episode 4: reward: -16.000, steps: 2639\n",
            "Episode 5: reward: -9.000, steps: 3284\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       26                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0198\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0206\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 4.244 - mean_q: 5.163 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 4.283 - mean_q: 5.211 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0214\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 4.279 - mean_q: 5.204 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0208\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 4.211 - mean_q: 5.124 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 754/5000 [===>..........................] - ETA: 1:37 - reward: -0.0265Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -14.000, steps: 3375\n",
            "Episode 2: reward: -13.000, steps: 3202\n",
            "Episode 3: reward: -11.000, steps: 2963\n",
            "Episode 4: reward: -11.000, steps: 3350\n",
            "Episode 5: reward: -15.000, steps: 2958\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       27                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0218\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0204\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 4.214 - mean_q: 5.126 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0204\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 4.157 - mean_q: 5.055 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0218\n",
            "6 episodes - episode_reward: -20.333 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 4.134 - mean_q: 5.029 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0212\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 4.126 - mean_q: 5.019 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 380/5000 [=>............................] - ETA: 1:45 - reward: -0.0132Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -16.000, steps: 2582\n",
            "Episode 2: reward: -17.000, steps: 2317\n",
            "Episode 3: reward: -19.000, steps: 2656\n",
            "Episode 4: reward: -15.000, steps: 2677\n",
            "Episode 5: reward: -12.000, steps: 2620\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       28                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0218\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0234\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0230\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 4.129 - mean_q: 5.025 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0180\n",
            "5 episodes - episode_reward: -19.400 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 4.110 - mean_q: 5.003 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0226\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 4.114 - mean_q: 5.006 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0198\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 4.147 - mean_q: 5.046 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 153/5000 [..............................] - ETA: 1:50 - reward: -0.0327Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -14.000, steps: 3065\n",
            "Episode 2: reward: -16.000, steps: 2442\n",
            "Episode 3: reward: -13.000, steps: 2816\n",
            "Episode 4: reward: -18.000, steps: 2404\n",
            "Episode 5: reward: -13.000, steps: 3050\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       29                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0222\n",
            "6 episodes - episode_reward: -20.333 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 4.159 - mean_q: 5.059 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 4.155 - mean_q: 5.055 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 4.168 - mean_q: 5.069 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0214\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 4.127 - mean_q: 5.019 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            "  44/5000 [..............................] - ETA: 1:48 - reward: -0.0227Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -14.000, steps: 2671\n",
            "Episode 2: reward: -16.000, steps: 2095\n",
            "Episode 3: reward: -17.000, steps: 2541\n",
            "Episode 4: reward: -14.000, steps: 2766\n",
            "Episode 5: reward: -16.000, steps: 2558\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       30                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0198\n",
            "5 episodes - episode_reward: -19.800 [-20.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0222\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0214\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 4.074 - mean_q: 4.957 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0236\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 4.045 - mean_q: 4.922 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0218\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 4.060 - mean_q: 4.940 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0184\n",
            "5 episodes - episode_reward: -19.200 [-21.000, -15.000] - loss: 0.010 - mean_absolute_error: 4.018 - mean_q: 4.888 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 387/5000 [=>............................] - ETA: 1:42 - reward: -0.0181Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -12.000, steps: 2765\n",
            "Episode 2: reward: -14.000, steps: 2768\n",
            "Episode 3: reward: -8.000, steps: 3450\n",
            "Episode 4: reward: -16.000, steps: 2721\n",
            "Episode 5: reward: -13.000, steps: 2917\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       31                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 4.003 - mean_q: 4.870 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0212\n",
            "6 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.932 - mean_q: 4.785 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.932 - mean_q: 4.785 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0186\n",
            "4 episodes - episode_reward: -18.750 [-20.000, -17.000] - loss: 0.011 - mean_absolute_error: 3.980 - mean_q: 4.843 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            "1162/5000 [=====>........................] - ETA: 1:26 - reward: -0.0155Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -10.000, steps: 3049\n",
            "Episode 2: reward: -9.000, steps: 3371\n",
            "Episode 3: reward: -4.000, steps: 3834\n",
            "Episode 4: reward: -13.000, steps: 2515\n",
            "Episode 5: reward: -14.000, steps: 2591\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       32                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0230\n",
            "6 episodes - episode_reward: -20.333 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0230\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.956 - mean_q: 4.813 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0226\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -20.000] - loss: 0.010 - mean_absolute_error: 3.926 - mean_q: 4.776 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.010 - mean_absolute_error: 3.858 - mean_q: 4.696 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4058/5000 [=======================>......] - ETA: 21s - reward: -0.0212Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -12.000, steps: 2794\n",
            "Episode 2: reward: -14.000, steps: 3052\n",
            "Episode 3: reward: -11.000, steps: 3040\n",
            "Episode 4: reward: -18.000, steps: 1603\n",
            "Episode 5: reward: -12.000, steps: 2730\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       33                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0222\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 3.842 - mean_q: 4.676 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0198\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - loss: 0.010 - mean_absolute_error: 3.822 - mean_q: 4.651 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0212\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.789 - mean_q: 4.610 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0210\n",
            "5 episodes - episode_reward: -19.800 [-20.000, -19.000] - loss: 0.011 - mean_absolute_error: 3.809 - mean_q: 4.637 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 799/5000 [===>..........................] - ETA: 1:35 - reward: -0.0175Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -12.000, steps: 3148\n",
            "Episode 2: reward: -19.000, steps: 2214\n",
            "Episode 3: reward: -19.000, steps: 2286\n",
            "Episode 4: reward: -13.000, steps: 3570\n",
            "Episode 5: reward: -14.000, steps: 3097\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       34                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 115s 23ms/step - reward: -0.0222\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - loss: 0.010 - mean_absolute_error: 3.874 - mean_q: 4.714 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0212\n",
            "6 episodes - episode_reward: -20.000 [-21.000, -17.000] - loss: 0.010 - mean_absolute_error: 3.884 - mean_q: 4.726 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - loss: 0.010 - mean_absolute_error: 3.845 - mean_q: 4.678 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 3.863 - mean_q: 4.702 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 349/5000 [=>............................] - ETA: 1:43 - reward: -0.0229Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -14.000, steps: 2376\n",
            "Episode 2: reward: -18.000, steps: 2212\n",
            "Episode 3: reward: -15.000, steps: 2516\n",
            "Episode 4: reward: -17.000, steps: 1991\n",
            "Episode 5: reward: -12.000, steps: 2645\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       35                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0188\n",
            "5 episodes - episode_reward: -19.800 [-20.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.864 - mean_q: 4.702 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0220\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 3.913 - mean_q: 4.763 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0224\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 3.920 - mean_q: 4.770 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.936 - mean_q: 4.791 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            " 522/5000 [==>...........................] - ETA: 1:38 - reward: -0.0153Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -16.000, steps: 2100\n",
            "Episode 2: reward: -13.000, steps: 2897\n",
            "Episode 3: reward: -16.000, steps: 2450\n",
            "Episode 4: reward: -10.000, steps: 3271\n",
            "Episode 5: reward: -14.000, steps: 2803\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       36                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0212\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0236\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -18.000] - loss: 0.010 - mean_absolute_error: 3.909 - mean_q: 4.756 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.835 - mean_q: 4.665 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0212\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.787 - mean_q: 4.610 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4520/5000 [==========================>...] - ETA: 10s - reward: -0.0226Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -18.000, steps: 2111\n",
            "Episode 2: reward: -10.000, steps: 3238\n",
            "Episode 3: reward: -15.000, steps: 2951\n",
            "Episode 4: reward: -13.000, steps: 2936\n",
            "Episode 5: reward: -15.000, steps: 2740\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       37                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0252\n",
            "6 episodes - episode_reward: -20.833 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.770 - mean_q: 4.588 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 114s 23ms/step - reward: -0.0218\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -18.000] - loss: 0.010 - mean_absolute_error: 3.732 - mean_q: 4.541 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0226\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - loss: 0.010 - mean_absolute_error: 3.747 - mean_q: 4.560 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4127/5000 [=======================>......] - ETA: 19s - reward: -0.0211Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -17.000, steps: 2693\n",
            "Episode 2: reward: -17.000, steps: 2338\n",
            "Episode 3: reward: -15.000, steps: 2800\n",
            "Episode 4: reward: -12.000, steps: 2670\n",
            "Episode 5: reward: -13.000, steps: 2654\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       38                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0234\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "6 episodes - episode_reward: -20.833 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0226\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - loss: 0.010 - mean_absolute_error: 3.804 - mean_q: 4.628 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0222\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.801 - mean_q: 4.624 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0224\n",
            "5 episodes - episode_reward: -20.800 [-21.000, -20.000] - loss: 0.009 - mean_absolute_error: 3.780 - mean_q: 4.598 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4094/5000 [=======================>......] - ETA: 20s - reward: -0.0220Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -18.000, steps: 2573\n",
            "Episode 2: reward: -12.000, steps: 3242\n",
            "Episode 3: reward: -14.000, steps: 2719\n",
            "Episode 4: reward: -18.000, steps: 2667\n",
            "Episode 5: reward: -17.000, steps: 2737\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       39                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0228\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0234\n",
            "6 episodes - episode_reward: -20.833 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 3.653 - mean_q: 4.446 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0210\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.009 - mean_absolute_error: 3.675 - mean_q: 4.473 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 113s 23ms/step - reward: -0.0222\n",
            "6 episodes - episode_reward: -20.167 [-21.000, -18.000] - loss: 0.009 - mean_absolute_error: 3.613 - mean_q: 4.395 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4627/5000 [==========================>...] - ETA: 8s - reward: -0.0214Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -14.000, steps: 2932\n",
            "Episode 2: reward: -17.000, steps: 2229\n",
            "Episode 3: reward: -17.000, steps: 2544\n",
            "Episode 4: reward: -12.000, steps: 2889\n",
            "Episode 5: reward: -15.000, steps: 2893\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       40                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0200\n",
            "4 episodes - episode_reward: -20.000 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0224\n",
            "6 episodes - episode_reward: -20.833 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0238\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.009 - mean_absolute_error: 3.569 - mean_q: 4.345 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0206\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.009 - mean_absolute_error: 3.569 - mean_q: 4.342 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0202\n",
            "5 episodes - episode_reward: -19.400 [-20.000, -18.000] - loss: 0.009 - mean_absolute_error: 3.611 - mean_q: 4.394 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4853/5000 [============================>.] - ETA: 3s - reward: -0.0239Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -18.000, steps: 2070\n",
            "Episode 2: reward: -12.000, steps: 2498\n",
            "Episode 3: reward: -15.000, steps: 2546\n",
            "Episode 4: reward: -13.000, steps: 3055\n",
            "Episode 5: reward: -12.000, steps: 2789\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       41                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0236\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0210\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0228\n",
            "6 episodes - episode_reward: -20.500 [-21.000, -19.000] - loss: 0.009 - mean_absolute_error: 3.572 - mean_q: 4.347 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0214\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -19.000] - loss: 0.009 - mean_absolute_error: 3.544 - mean_q: 4.313 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0212\n",
            "6 episodes - episode_reward: -20.333 [-21.000, -20.000] - loss: 0.009 - mean_absolute_error: 3.506 - mean_q: 4.268 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4365/5000 [=========================>....] - ETA: 14s - reward: -0.0222Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -16.000, steps: 1940\n",
            "Episode 2: reward: -17.000, steps: 2189\n",
            "Episode 3: reward: -11.000, steps: 2843\n",
            "Episode 4: reward: -13.000, steps: 2558\n",
            "Episode 5: reward: -19.000, steps: 2316\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       42                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0244\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0226\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0208\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.009 - mean_absolute_error: 3.543 - mean_q: 4.313 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0206\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.009 - mean_absolute_error: 3.574 - mean_q: 4.351 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0226\n",
            "6 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.009 - mean_absolute_error: 3.552 - mean_q: 4.322 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "3891/5000 [======================>.......] - ETA: 24s - reward: -0.0244Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -17.000, steps: 2218\n",
            "Episode 2: reward: -16.000, steps: 2592\n",
            "Episode 3: reward: -12.000, steps: 2743\n",
            "Episode 4: reward: -12.000, steps: 3365\n",
            "Episode 5: reward: -12.000, steps: 3255\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       43                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0222\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0234\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0216\n",
            "5 episodes - episode_reward: -20.000 [-21.000, -19.000] - loss: 0.009 - mean_absolute_error: 3.531 - mean_q: 4.297 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0224\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.009 - mean_absolute_error: 3.493 - mean_q: 4.249 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0236\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.009 - mean_absolute_error: 3.428 - mean_q: 4.170 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4122/5000 [=======================>......] - ETA: 19s - reward: -0.0211Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -17.000, steps: 2307\n",
            "Episode 2: reward: -12.000, steps: 3000\n",
            "Episode 3: reward: -18.000, steps: 2227\n",
            "Episode 4: reward: -13.000, steps: 2444\n",
            "Episode 5: reward: -14.000, steps: 2360\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       44                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0186\n",
            "4 episodes - episode_reward: -19.250 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0248\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0194\n",
            "5 episodes - episode_reward: -19.200 [-21.000, -16.000] - loss: 0.009 - mean_absolute_error: 3.375 - mean_q: 4.109 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0234\n",
            "6 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.008 - mean_absolute_error: 3.363 - mean_q: 4.094 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0238\n",
            "6 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.009 - mean_absolute_error: 3.380 - mean_q: 4.114 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4515/5000 [==========================>...] - ETA: 10s - reward: -0.0213Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -18.000, steps: 2375\n",
            "Episode 2: reward: -16.000, steps: 2475\n",
            "Episode 3: reward: -14.000, steps: 2630\n",
            "Episode 4: reward: -15.000, steps: 3086\n",
            "Episode 5: reward: -16.000, steps: 2614\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                       45                         **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "**                                                 **\n",
            "*****************************************************\n",
            "*****************************************************\n",
            "Training for 5000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0234\n",
            "5 episodes - episode_reward: -20.400 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 17s 3ms/step - reward: -0.0182\n",
            "5 episodes - episode_reward: -19.400 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 110s 22ms/step - reward: -0.0214\n",
            "5 episodes - episode_reward: -20.200 [-21.000, -18.000] - loss: 0.008 - mean_absolute_error: 3.384 - mean_q: 4.118 - mean_eps: 0.989 - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "5000/5000 [==============================] - 111s 22ms/step - reward: -0.0238\n",
            "6 episodes - episode_reward: -21.000 [-21.000, -21.000] - loss: 0.008 - mean_absolute_error: 3.369 - mean_q: 4.099 - mean_eps: 0.984 - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0204\n",
            "5 episodes - episode_reward: -19.800 [-21.000, -18.000] - loss: 0.008 - mean_absolute_error: 3.320 - mean_q: 4.039 - mean_eps: 0.980 - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 112s 22ms/step - reward: -0.0220\n",
            "5 episodes - episode_reward: -20.600 [-21.000, -20.000] - loss: 0.008 - mean_absolute_error: 3.297 - mean_q: 4.013 - mean_eps: 0.975 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            "  65/5000 [..............................] - ETA: 1:53 - reward: -0.0154Error al entrenar, se ha interrumpido el fit\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -14.000, steps: 2644\n",
            "Episode 2: reward: -15.000, steps: 2490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pNCgyU12pUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn.save_weights(BASE_FOLDER + weights_filename, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-vBgn_sehsw",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hm_5gKgekwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eposidiosEvaluar = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPW1aaSfUVZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rta = dqn.test(env, nb_episodes=eposidiosEvaluar, visualize=graficosVisualesGame).history['episode_reward'][1]\n",
        "\n",
        "print(rta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdWvKm4pLiS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}