{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nuevo editor de código de Colab",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wisrovi/RedesNeuronales/blob/master/AprendizajePorRefuerzo-PongDeterministic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDELHJVDYxoA",
        "colab_type": "text"
      },
      "source": [
        "# Teoría"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMDUzanRLoUS",
        "colab_type": "text"
      },
      "source": [
        "## Objetivo: \n",
        "Repasar los conceptos vistos en clase.\n",
        "\n",
        "La puntuación de este bloque es de 4 puntos sobre la nota final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMo0eBLY1Gs",
        "colab_type": "text"
      },
      "source": [
        "**Define brevemente qué es el aprendizaje por refuerzo. ¿Qué diferencias hay entre aprendizaje supervisado, no supervisado y por refuerzo?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izoGROmUY1OO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lug2H4ygZCGU",
        "colab_type": "text"
      },
      "source": [
        "**Define con tus palabras los conceptos de Entorno, Agente, Recompensa, Estado y Observación.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd5veW0eZCO7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6GMFSLZCLw",
        "colab_type": "text"
      },
      "source": [
        "**Dependiendo del algoritmo de aprendizaje por refuerzo que se use, ¿qué clasificaciones podemos encontrar? Coméntalas brevemente.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cW1RBOpZCJ-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw-6C3J3Y1MJ",
        "colab_type": "text"
      },
      "source": [
        "**Lista tres diferencias entre los algoritmos de DQN y Policy Gradient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22_VeKRfZYXN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98NdMWLxaQv-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FwICPIVaQ8N",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iTYuIoLawAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_zXk0PXZYad",
        "colab_type": "text"
      },
      "source": [
        "# Práctica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkgoJNjdZ6U2",
        "colab_type": "text"
      },
      "source": [
        "Objetivo: Implementar una solución, usando keras-rl y basada en el algoritmo de DQN visto en clase, para que un agente aprenda una estrategia ganadora en el juego del Pong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI0finZIZ6R9",
        "colab_type": "text"
      },
      "source": [
        "La puntuación de este bloque es de 6 puntos sobre la nota final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXpPfxW3Y1Jw",
        "colab_type": "text"
      },
      "source": [
        "El entorno sobre el que trabajaremos será _PongDeterministic-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "Para evaluar cómo lo está haciendo el agente, la recompensa en el _Pong_ oscila, aproximadamente, en el rango de valores **[-20, 20]**. La estrategia óptima de un agente estaría alrededor de una media de recompensa de 20.\n",
        "\n",
        "- **NOTA IMPORTANTE**: Si el agente no llegara a aprender una estrategia ganadora, responder sobre la mejor estrategia obtenida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy3zuNXdwIsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "7799e713-b121-4fe4-9fb0-442835f9a01a"
      },
      "source": [
        "#para instalar la insterfaz grafica\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "\n",
        "#para rendereizar datos\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "\n",
        "#para activar la interfaz visual para entrenar el agente\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "#para instalar el visual de atari\n",
        "!pip install “gym[atari]\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.3).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.4)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.7)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet) (1.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet) (19.3.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet) (1.6.2)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (1.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (0.33.6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `\"'\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzds6S7gvKYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPRbpTdwLE8T",
        "colab_type": "code",
        "outputId": "3bb9ff51-c459-4637-b565-8dc85455b534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install keras-rl==0.4.2\n",
        "!pip install tensorflow==1.13.1\n",
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "!pip install h5py\n",
        "\n",
        "\n",
        "#creo son necesarias, pero no se ha comprobado que en su ausencia falle\n",
        "!pip install jupyter\n",
        "!pip install torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl==0.4.2) (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.17.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (2.8.0)\n",
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.6.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym) (3.4.7.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.4)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.4.7.28)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari]) (0.46)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter) (4.6.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter) (4.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter) (7.5.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.2.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (3.1.0)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (4.4.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (1.4.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.8.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (2.1.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (4.3.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (2.10.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (4.6.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.3)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter) (5.3.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter) (4.5.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter) (3.5.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter) (1.0.18)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (0.8.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter) (1.12.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter) (2.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->jupyter) (4.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter) (1.1.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter) (4.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter) (41.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter) (0.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbyoXvaEYwuT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "810a5e3a-0500-42d8-8169-7899e1c87b72"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JwqwV1RUO6x",
        "colab_type": "code",
        "outputId": "c2aa2e41-dda5-4e3e-e0d5-a1b4ffbd6222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ENV_NAME = 'PongDeterministic-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym.make(ENV_NAME)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# random seed\n",
        "np.random.seed(123)\n",
        "env.seed(123)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[123, 151010689]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWfdcMqFUO-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the input shape to resize the screen\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "# This processor will be similar to the Atari processor\n",
        "class PongProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        \n",
        "        img = Image.fromarray(observation)\n",
        "        # resize and convert to grayscale\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        \n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6QI3zYOLOwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor = PongProcessor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6GlW1ZUUO8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createModel():\n",
        "    input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "\n",
        "    model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(nb_actions))\n",
        "    model.add(Activation('linear'))\n",
        "\n",
        "    model.add(Dense(nb_actions))\n",
        "    model.add(Activation('linear'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc9Tqn5sLT1z",
        "colab_type": "code",
        "outputId": "020eb625-ed7d-4d94-8404-8c9d2f773b0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        }
      },
      "source": [
        "model = createModel()\n",
        "print(model.summary())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 3078      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 6)                 0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 42        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 1,687,248\n",
            "Trainable params: 1,687,248\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni1mhPqvUO32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
        "                              nb_steps=1000000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhrHpy3UUVSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn = DQNAgent(\n",
        "    nb_actions=nb_actions, \n",
        "    model=model, \n",
        "    policy=policy, \n",
        "    memory=memory,\n",
        "    processor=processor, \n",
        "    nb_steps_warmup=50000, \n",
        "    target_model_update=10000,\n",
        "    gamma=.99,\n",
        "    train_interval=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_IJhPfUUVVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compilar modelo\n",
        "optimizer = Adam(lr=1e-3)\n",
        "dqn.compile(optimizer, metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg9-i8fhUVb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#callback\n",
        "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log.json'.format(ENV_NAME)\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=50000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5ZK9k8REljM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF2uVIh-El8N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a625615a-9699-42cb-f7a6-80023eb2f3eb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "BASE_FOLDER = '/content/gdrive/My Drive/Master IA/AprendizajePorRefuerzo/'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJVHCGUMUVe2",
        "colab_type": "code",
        "outputId": "5f601540-1e68-4ff7-ff3f-0c5cc2b0a0d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "weights_filename = 'dqn_' + ENV_NAME + '_weights.h5f'\n",
        "scoreOld = -50\n",
        "usarPesosEntrenados = True\n",
        "if usarPesosEntrenados:\n",
        "    try:\n",
        "        dqn.load_weights(BASE_FOLDER + weights_filename)\n",
        "        scoreOld = dqn.test(env, nb_episodes=2, visualize=False).history['episode_reward'][1]\n",
        "        print(dqn.test(env, nb_episodes=2, visualize=False).history['episode_reward'][1])\n",
        "    except:\n",
        "        print(\"No se encontro archivos de pesos\")\n",
        "        pass\n",
        "\n",
        "entrenar = True\n",
        "agenteOld = None\n",
        "if entrenar:\n",
        "    conteo = 0\n",
        "    for i in range(50):\n",
        "    # Training part\n",
        "        try:\n",
        "            dqn.fit(env, callbacks=callbacks, nb_steps=100000, log_interval=10000, visualize=False)\n",
        "        except:\n",
        "            print(\"Erro al entrenar\")\n",
        "            pass\n",
        "\n",
        "        testEval = dqn.test(env, nb_episodes=2, visualize=False).history['episode_reward'][1]\n",
        "        dqn.save_weights(BASE_FOLDER + weights_filename, overwrite=True)\n",
        "\n",
        "        if scoreOld < testEval:\n",
        "            scoreOld = testEval\n",
        "            agenteOld = dqn\n",
        "            dqn.save_weights(BASE_FOLDER + weights_filename, overwrite=True)\n",
        "            print(\"Guardando pesos\", scoreOld)\n",
        "        else:\n",
        "            print(\"No hubo mejora.\")\n",
        "            conteo += 1\n",
        "            if conteo > 3:\n",
        "                pass\n",
        "                #break\n",
        "            #dqn = agenteOld        \n",
        "else:    \n",
        "    dqn.load_weights(weights_filename)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "-21.0\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0229\n",
            "11 episodes - episode_reward: -20.545 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0213\n",
            "10 episodes - episode_reward: -20.500 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0210\n",
            "11 episodes - episode_reward: -19.727 [-21.000, -16.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4562/10000 [============>.................] - ETA: 21s - reward: -0.0215Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0215\n",
            "10 episodes - episode_reward: -20.100 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0210\n",
            "11 episodes - episode_reward: -20.000 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0204\n",
            "10 episodes - episode_reward: -19.600 [-21.000, -15.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 5192/10000 [==============>...............] - ETA: 18s - reward: -0.0208Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0238\n",
            "11 episodes - episode_reward: -20.636 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0209\n",
            "11 episodes - episode_reward: -20.000 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0223\n",
            "10 episodes - episode_reward: -20.300 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 3723/10000 [==========>...................] - ETA: 24s - reward: -0.0220Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0214\n",
            "10 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0228\n",
            "11 episodes - episode_reward: -20.545 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0218\n",
            "11 episodes - episode_reward: -20.182 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4227/10000 [===========>..................] - ETA: 22s - reward: -0.0218Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0227\n",
            "11 episodes - episode_reward: -20.636 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0218\n",
            "10 episodes - episode_reward: -20.300 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0204\n",
            "11 episodes - episode_reward: -19.818 [-21.000, -17.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4281/10000 [===========>..................] - ETA: 22s - reward: -0.0236Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0205\n",
            "10 episodes - episode_reward: -19.900 [-21.000, -17.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0224\n",
            "11 episodes - episode_reward: -20.273 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0228\n",
            "11 episodes - episode_reward: -20.636 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4574/10000 [============>.................] - ETA: 21s - reward: -0.0197Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0214\n",
            "10 episodes - episode_reward: -20.200 [-21.000, -17.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0217\n",
            "11 episodes - episode_reward: -19.909 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0232\n",
            "11 episodes - episode_reward: -20.545 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4187/10000 [===========>..................] - ETA: 23s - reward: -0.0196Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0224\n",
            "10 episodes - episode_reward: -20.400 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0221\n",
            "11 episodes - episode_reward: -20.545 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0213\n",
            "11 episodes - episode_reward: -20.091 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4718/10000 [=============>................] - ETA: 21s - reward: -0.0195Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0222\n",
            "10 episodes - episode_reward: -20.300 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0218\n",
            "11 episodes - episode_reward: -20.182 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0210\n",
            "11 episodes - episode_reward: -20.091 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4236/10000 [===========>..................] - ETA: 22s - reward: -0.0229Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0244\n",
            "11 episodes - episode_reward: -20.818 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0217\n",
            "11 episodes - episode_reward: -20.636 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0212\n",
            "10 episodes - episode_reward: -20.100 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 3769/10000 [==========>...................] - ETA: 24s - reward: -0.0233Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0228\n",
            "11 episodes - episode_reward: -20.636 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0210\n",
            "10 episodes - episode_reward: -20.300 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0236\n",
            "11 episodes - episode_reward: -20.636 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 3563/10000 [=========>....................] - ETA: 25s - reward: -0.0233Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0202\n",
            "10 episodes - episode_reward: -20.100 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0227\n",
            "11 episodes - episode_reward: -20.545 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0219\n",
            "10 episodes - episode_reward: -20.200 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4656/10000 [============>.................] - ETA: 21s - reward: -0.0219Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0215\n",
            "10 episodes - episode_reward: -20.200 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0224\n",
            "11 episodes - episode_reward: -20.636 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0216\n",
            "11 episodes - episode_reward: -20.273 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4632/10000 [============>.................] - ETA: 21s - reward: -0.0203Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0231\n",
            "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0210\n",
            "10 episodes - episode_reward: -20.100 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0219\n",
            "11 episodes - episode_reward: -20.545 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4139/10000 [===========>..................] - ETA: 23s - reward: -0.0232Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0220\n",
            "10 episodes - episode_reward: -20.400 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0211\n",
            "11 episodes - episode_reward: -20.182 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0223\n",
            "11 episodes - episode_reward: -20.273 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4599/10000 [============>.................] - ETA: 21s - reward: -0.0202Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0222\n",
            "10 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0219\n",
            "11 episodes - episode_reward: -20.364 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 39s 4ms/step - reward: -0.0218\n",
            "11 episodes - episode_reward: -20.545 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4620/10000 [============>.................] - ETA: 21s - reward: -0.0206Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0221\n",
            "10 episodes - episode_reward: -20.200 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0222\n",
            "11 episodes - episode_reward: -20.636 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0220\n",
            "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4097/10000 [===========>..................] - ETA: 23s - reward: -0.0222Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0223\n",
            "10 episodes - episode_reward: -20.400 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0202\n",
            "11 episodes - episode_reward: -20.091 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0219\n",
            "10 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4794/10000 [=============>................] - ETA: 21s - reward: -0.0221Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0218\n",
            "10 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0218\n",
            "11 episodes - episode_reward: -20.273 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0219\n",
            "11 episodes - episode_reward: -20.545 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4323/10000 [===========>..................] - ETA: 23s - reward: -0.0231Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0219\n",
            "10 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0219\n",
            "11 episodes - episode_reward: -20.364 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0213\n",
            "11 episodes - episode_reward: -20.091 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4640/10000 [============>.................] - ETA: 21s - reward: -0.0211Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0247\n",
            "11 episodes - episode_reward: -20.818 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0233\n",
            "12 episodes - episode_reward: -20.667 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0234\n",
            "11 episodes - episode_reward: -20.818 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 2275/10000 [=====>........................] - ETA: 31s - reward: -0.0237Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0226\n",
            "11 episodes - episode_reward: -20.545 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0199\n",
            "10 episodes - episode_reward: -19.800 [-21.000, -17.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0223\n",
            "11 episodes - episode_reward: -20.364 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4553/10000 [============>.................] - ETA: 22s - reward: -0.0215Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0198\n",
            "10 episodes - episode_reward: -19.800 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0214\n",
            "10 episodes - episode_reward: -20.200 [-21.000, -17.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 40s 4ms/step - reward: -0.0223\n",
            "11 episodes - episode_reward: -20.818 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 4971/10000 [=============>................] - ETA: 20s - reward: -0.0235Erro al entrenar\n",
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "No hubo mejora.\n",
            "Training for 100000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            " 6289/10000 [=================>............] - ETA: 14s - reward: -0.0204Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPW1aaSfUVZ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "cadee52f-d7ef-4d5e-bb26-08d479330caa"
      },
      "source": [
        "print(dqn.test(env, nb_episodes=2, visualize=False).history['episode_reward'][1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 2 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "-21.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3D6p9q5ymoJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "fe21aa47-2c0f-41d9-8088-5afd0429c409"
      },
      "source": [
        "#obs0 = env.reset()\n",
        "#print(\"initial observation code:\", obs0)\n",
        "\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(210, 160, 3)\n",
            "Action space: Discrete(6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naWqkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}