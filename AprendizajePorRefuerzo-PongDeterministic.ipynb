{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nuevo editor de código de Colab",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wisrovi/RedesNeuronales/blob/master/AprendizajePorRefuerzo-PongDeterministic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDELHJVDYxoA",
        "colab_type": "text"
      },
      "source": [
        "# Teoría"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMDUzanRLoUS",
        "colab_type": "text"
      },
      "source": [
        "## Objetivo: \n",
        "Repasar los conceptos vistos en clase.\n",
        "\n",
        "La puntuación de este bloque es de 4 puntos sobre la nota final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AMo0eBLY1Gs",
        "colab_type": "text"
      },
      "source": [
        "**Define brevemente qué es el aprendizaje por refuerzo. ¿Qué diferencias hay entre aprendizaje supervisado, no supervisado y por refuerzo?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izoGROmUY1OO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lug2H4ygZCGU",
        "colab_type": "text"
      },
      "source": [
        "**Define con tus palabras los conceptos de Entorno, Agente, Recompensa, Estado y Observación.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd5veW0eZCO7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6GMFSLZCLw",
        "colab_type": "text"
      },
      "source": [
        "**Dependiendo del algoritmo de aprendizaje por refuerzo que se use, ¿qué clasificaciones podemos encontrar? Coméntalas brevemente.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cW1RBOpZCJ-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw-6C3J3Y1MJ",
        "colab_type": "text"
      },
      "source": [
        "**Lista tres diferencias entre los algoritmos de DQN y Policy Gradient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22_VeKRfZYXN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98NdMWLxaQv-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FwICPIVaQ8N",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iTYuIoLawAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_zXk0PXZYad",
        "colab_type": "text"
      },
      "source": [
        "# Práctica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkgoJNjdZ6U2",
        "colab_type": "text"
      },
      "source": [
        "Objetivo: Implementar una solución, usando keras-rl y basada en el algoritmo de DQN visto en clase, para que un agente aprenda una estrategia ganadora en el juego del Pong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI0finZIZ6R9",
        "colab_type": "text"
      },
      "source": [
        "La puntuación de este bloque es de 6 puntos sobre la nota final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXpPfxW3Y1Jw",
        "colab_type": "text"
      },
      "source": [
        "El entorno sobre el que trabajaremos será _PongDeterministic-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "Para evaluar cómo lo está haciendo el agente, la recompensa en el _Pong_ oscila, aproximadamente, en el rango de valores **[-20, 20]**. La estrategia óptima de un agente estaría alrededor de una media de recompensa de 20.\n",
        "\n",
        "- **NOTA IMPORTANTE**: Si el agente no llegara a aprender una estrategia ganadora, responder sobre la mejor estrategia obtenida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPRbpTdwLE8T",
        "colab_type": "code",
        "outputId": "6bd4e346-0f99-421f-c506-e1acb4ee84c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install keras-rl==0.4.2\n",
        "!pip install tensorflow==1.13.1\n",
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "!pip install h5py\n",
        "\n",
        "\n",
        "#creo son necesarias, pero no se ha comprobado que en su ausencia falle\n",
        "!pip install jupyter\n",
        "!pip install torch"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl==0.4.2) (2.2.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.3.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.17.4)\n",
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.4)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym) (3.4.7.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.15.4)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.17.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.4.7.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.2)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari]) (0.46)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter) (7.5.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter) (4.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.2.2)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter) (4.6.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter) (5.2.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (3.1.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (2.1.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (2.10.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (4.6.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (1.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (4.3.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.8.4)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (4.4.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter) (0.6.0)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter) (3.5.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter) (4.5.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter) (5.3.4)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (0.8.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter) (1.0.18)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter) (1.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter) (1.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->jupyter) (4.4.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter) (2.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (4.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter) (41.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->jupyter) (2.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->jupyter) (17.0.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter) (0.1.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbyoXvaEYwuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JwqwV1RUO6x",
        "colab_type": "code",
        "outputId": "4152bede-3feb-4fa1-e0aa-fb2e597e780c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ENV_NAME = 'PongDeterministic-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym.make(ENV_NAME)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# random seed\n",
        "np.random.seed(123)\n",
        "env.seed(123)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[123, 151010689]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWfdcMqFUO-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the input shape to resize the screen\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "# This processor will be similar to the Atari processor\n",
        "class PongProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        \n",
        "        img = Image.fromarray(observation)\n",
        "        # resize and convert to grayscale\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        \n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6QI3zYOLOwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor = PongProcessor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6GlW1ZUUO8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createModel():\n",
        "    input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "\n",
        "    model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(nb_actions))\n",
        "    model.add(Activation('linear'))\n",
        "\n",
        "    model.add(Dense(nb_actions))\n",
        "    model.add(Activation('linear'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc9Tqn5sLT1z",
        "colab_type": "code",
        "outputId": "925cb2cb-98d8-483b-8e5d-ed2ce57e0ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "model = createModel()\n",
        "print(model.summary())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_3 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 6)                 3078      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 6)                 0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 6)                 42        \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 1,687,248\n",
            "Trainable params: 1,687,248\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni1mhPqvUO32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
        "                              nb_steps=1000000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhrHpy3UUVSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn = DQNAgent(\n",
        "    nb_actions=nb_actions, \n",
        "    model=model, \n",
        "    policy=policy, \n",
        "    memory=memory,\n",
        "    processor=processor, \n",
        "    nb_steps_warmup=50000, \n",
        "    target_model_update=10000,\n",
        "    gamma=.99,\n",
        "    train_interval=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_IJhPfUUVVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compilar modelo\n",
        "optimizer = Adam(lr=1e-3)\n",
        "dqn.compile(optimizer, metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg9-i8fhUVb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#callback\n",
        "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log.json'.format(ENV_NAME)\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=50000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJVHCGUMUVe2",
        "colab_type": "code",
        "outputId": "878d73bd-bf89-4205-cc2a-cf33b7b2ec11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training part\n",
        "#dqn.fit(env, callbacks=callbacks, nb_steps=500000, log_interval=10000, visualize=False)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 500000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 45s 5ms/step - reward: -0.0219\n",
            "10 episodes - episode_reward: -20.100 [-21.000, -17.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 47s 5ms/step - reward: -0.0200\n",
            "11 episodes - episode_reward: -19.727 [-21.000, -17.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 47s 5ms/step - reward: -0.0219\n",
            "10 episodes - episode_reward: -20.400 [-21.000, -19.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 47s 5ms/step - reward: -0.0237\n",
            "12 episodes - episode_reward: -20.333 [-21.000, -18.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 45s 5ms/step - reward: -0.0238\n",
            "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - ale.lives: 0.000\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "   13/10000 [..............................] - ETA: 49s - reward: -0.0769    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "10000/10000 [==============================] - 138s 14ms/step - reward: -0.0221\n",
            "11 episodes - episode_reward: -20.273 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 0.034 - mean_q: -0.010 - mean_eps: 0.950 - ale.lives: 0.000\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 140s 14ms/step - reward: -0.0210\n",
            "11 episodes - episode_reward: -20.182 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.062 - mean_q: -0.052 - mean_eps: 0.942 - ale.lives: 0.000\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 143s 14ms/step - reward: -0.0223\n",
            "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.084 - mean_q: -0.081 - mean_eps: 0.933 - ale.lives: 0.000\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 144s 14ms/step - reward: -0.0242\n",
            "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - loss: 0.010 - mean_absolute_error: 0.108 - mean_q: -0.112 - mean_eps: 0.924 - ale.lives: 0.000\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 144s 14ms/step - reward: -0.0207\n",
            "10 episodes - episode_reward: -20.200 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.129 - mean_q: -0.134 - mean_eps: 0.915 - ale.lives: 0.000\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 144s 14ms/step - reward: -0.0228\n",
            "12 episodes - episode_reward: -20.500 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.156 - mean_q: -0.163 - mean_eps: 0.906 - ale.lives: 0.000\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 139s 14ms/step - reward: -0.0223\n",
            "11 episodes - episode_reward: -20.364 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.176 - mean_q: -0.190 - mean_eps: 0.897 - ale.lives: 0.000\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 146s 15ms/step - reward: -0.0222\n",
            "10 episodes - episode_reward: -20.400 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.193 - mean_q: -0.209 - mean_eps: 0.888 - ale.lives: 0.000\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 140s 14ms/step - reward: -0.0244\n",
            "12 episodes - episode_reward: -20.833 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.202 - mean_q: -0.222 - mean_eps: 0.879 - ale.lives: 0.000\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 145s 14ms/step - reward: -0.0225\n",
            "11 episodes - episode_reward: -20.273 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.215 - mean_q: -0.238 - mean_eps: 0.870 - ale.lives: 0.000\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 142s 14ms/step - reward: -0.0219\n",
            "11 episodes - episode_reward: -20.273 [-21.000, -19.000] - loss: 0.010 - mean_absolute_error: 0.218 - mean_q: -0.244 - mean_eps: 0.861 - ale.lives: 0.000\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 138s 14ms/step - reward: -0.0230\n",
            "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.242 - mean_q: -0.270 - mean_eps: 0.852 - ale.lives: 0.000\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 146s 15ms/step - reward: -0.0233\n",
            "12 episodes - episode_reward: -20.333 [-21.000, -17.000] - loss: 0.011 - mean_absolute_error: 0.262 - mean_q: -0.292 - mean_eps: 0.843 - ale.lives: 0.000\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 138s 14ms/step - reward: -0.0211\n",
            "10 episodes - episode_reward: -20.300 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.290 - mean_q: -0.327 - mean_eps: 0.834 - ale.lives: 0.000\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 138s 14ms/step - reward: -0.0230\n",
            "11 episodes - episode_reward: -20.091 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.299 - mean_q: -0.336 - mean_eps: 0.825 - ale.lives: 0.000\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 141s 14ms/step - reward: -0.0220\n",
            "11 episodes - episode_reward: -20.727 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.321 - mean_q: -0.365 - mean_eps: 0.816 - ale.lives: 0.000\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 138s 14ms/step - reward: -0.0228\n",
            "11 episodes - episode_reward: -20.273 [-21.000, -18.000] - loss: 0.012 - mean_absolute_error: 0.336 - mean_q: -0.381 - mean_eps: 0.807 - ale.lives: 0.000\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 133s 13ms/step - reward: -0.0237\n",
            "12 episodes - episode_reward: -20.667 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.351 - mean_q: -0.399 - mean_eps: 0.798 - ale.lives: 0.000\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 135s 13ms/step - reward: -0.0249\n",
            "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.351 - mean_q: -0.400 - mean_eps: 0.789 - ale.lives: 0.000\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 136s 14ms/step - reward: -0.0237\n",
            "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.375 - mean_q: -0.428 - mean_eps: 0.780 - ale.lives: 0.000\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 137s 14ms/step - reward: -0.0240\n",
            "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.387 - mean_q: -0.444 - mean_eps: 0.771 - ale.lives: 0.000\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 137s 14ms/step - reward: -0.0236\n",
            "11 episodes - episode_reward: -20.818 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.396 - mean_q: -0.452 - mean_eps: 0.762 - ale.lives: 0.000\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 133s 13ms/step - reward: -0.0233\n",
            "12 episodes - episode_reward: -20.250 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.435 - mean_q: -0.499 - mean_eps: 0.753 - ale.lives: 0.000\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 134s 13ms/step - reward: -0.0236\n",
            "12 episodes - episode_reward: -20.250 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.460 - mean_q: -0.527 - mean_eps: 0.744 - ale.lives: 0.000\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 130s 13ms/step - reward: -0.0224\n",
            "11 episodes - episode_reward: -20.364 [-21.000, -18.000] - loss: 0.012 - mean_absolute_error: 0.484 - mean_q: -0.557 - mean_eps: 0.735 - ale.lives: 0.000\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 129s 13ms/step - reward: -0.0228\n",
            "11 episodes - episode_reward: -20.273 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.494 - mean_q: -0.570 - mean_eps: 0.726 - ale.lives: 0.000\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 132s 13ms/step - reward: -0.0229\n",
            "11 episodes - episode_reward: -20.364 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.517 - mean_q: -0.599 - mean_eps: 0.717 - ale.lives: 0.000\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 139s 14ms/step - reward: -0.0225\n",
            "11 episodes - episode_reward: -20.364 [-21.000, -18.000] - loss: 0.012 - mean_absolute_error: 0.542 - mean_q: -0.628 - mean_eps: 0.708 - ale.lives: 0.000\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 136s 14ms/step - reward: -0.0236\n",
            "12 episodes - episode_reward: -20.583 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.557 - mean_q: -0.647 - mean_eps: 0.699 - ale.lives: 0.000\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 135s 13ms/step - reward: -0.0246\n",
            "11 episodes - episode_reward: -20.636 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.575 - mean_q: -0.668 - mean_eps: 0.690 - ale.lives: 0.000\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 136s 14ms/step - reward: -0.0235\n",
            "12 episodes - episode_reward: -20.667 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.575 - mean_q: -0.668 - mean_eps: 0.681 - ale.lives: 0.000\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 130s 13ms/step - reward: -0.0233\n",
            "11 episodes - episode_reward: -20.364 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.584 - mean_q: -0.680 - mean_eps: 0.672 - ale.lives: 0.000\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 130s 13ms/step - reward: -0.0247\n",
            "12 episodes - episode_reward: -20.750 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.587 - mean_q: -0.680 - mean_eps: 0.663 - ale.lives: 0.000\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 131s 13ms/step - reward: -0.0247\n",
            "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.584 - mean_q: -0.680 - mean_eps: 0.654 - ale.lives: 0.000\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 130s 13ms/step - reward: -0.0246\n",
            "12 episodes - episode_reward: -20.667 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.596 - mean_q: -0.694 - mean_eps: 0.645 - ale.lives: 0.000\n",
            "\n",
            "Interval 41 (400000 steps performed)\n",
            "10000/10000 [==============================] - 132s 13ms/step - reward: -0.0241\n",
            "12 episodes - episode_reward: -20.500 [-21.000, -20.000] - loss: 0.011 - mean_absolute_error: 0.628 - mean_q: -0.730 - mean_eps: 0.636 - ale.lives: 0.000\n",
            "\n",
            "Interval 42 (410000 steps performed)\n",
            "10000/10000 [==============================] - 131s 13ms/step - reward: -0.0221\n",
            "11 episodes - episode_reward: -20.182 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.649 - mean_q: -0.757 - mean_eps: 0.627 - ale.lives: 0.000\n",
            "\n",
            "Interval 43 (420000 steps performed)\n",
            "10000/10000 [==============================] - 131s 13ms/step - reward: -0.0243\n",
            "11 episodes - episode_reward: -20.636 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.661 - mean_q: -0.769 - mean_eps: 0.618 - ale.lives: 0.000\n",
            "\n",
            "Interval 44 (430000 steps performed)\n",
            "10000/10000 [==============================] - 133s 13ms/step - reward: -0.0244\n",
            "12 episodes - episode_reward: -20.583 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.679 - mean_q: -0.791 - mean_eps: 0.609 - ale.lives: 0.000\n",
            "\n",
            "Interval 45 (440000 steps performed)\n",
            "10000/10000 [==============================] - 131s 13ms/step - reward: -0.0230\n",
            "12 episodes - episode_reward: -20.333 [-21.000, -18.000] - loss: 0.011 - mean_absolute_error: 0.702 - mean_q: -0.819 - mean_eps: 0.600 - ale.lives: 0.000\n",
            "\n",
            "Interval 46 (450000 steps performed)\n",
            "10000/10000 [==============================] - 139s 14ms/step - reward: -0.0230\n",
            "11 episodes - episode_reward: -20.273 [-21.000, -19.000] - loss: 0.013 - mean_absolute_error: 0.705 - mean_q: -0.821 - mean_eps: 0.591 - ale.lives: 0.000\n",
            "\n",
            "Interval 47 (460000 steps performed)\n",
            "10000/10000 [==============================] - 132s 13ms/step - reward: -0.0229\n",
            "11 episodes - episode_reward: -20.455 [-21.000, -19.000] - loss: 0.012 - mean_absolute_error: 0.732 - mean_q: -0.857 - mean_eps: 0.582 - ale.lives: 0.000\n",
            "\n",
            "Interval 48 (470000 steps performed)\n",
            "10000/10000 [==============================] - 137s 14ms/step - reward: -0.0253\n",
            "12 episodes - episode_reward: -20.917 [-21.000, -20.000] - loss: 0.012 - mean_absolute_error: 0.749 - mean_q: -0.874 - mean_eps: 0.573 - ale.lives: 0.000\n",
            "\n",
            "Interval 49 (480000 steps performed)\n",
            "10000/10000 [==============================] - 136s 14ms/step - reward: -0.0237\n",
            "12 episodes - episode_reward: -20.417 [-21.000, -19.000] - loss: 0.011 - mean_absolute_error: 0.764 - mean_q: -0.895 - mean_eps: 0.564 - ale.lives: 0.000\n",
            "\n",
            "Interval 50 (490000 steps performed)\n",
            "10000/10000 [==============================] - 134s 13ms/step - reward: -0.0236\n",
            "done, took 6374.687 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f37c65fdb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcwoc6ozOlUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights_filename = 'dqn_' + ENV_NAME + '_weights.h5f'\n",
        "\n",
        "dqn.save_weights(weights_filename, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPW1aaSfUVZ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "dc6a1b97-31aa-4720-da0a-9cf70365d961"
      },
      "source": [
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "Episode 3: reward: -21.000, steps: 764\n",
            "Episode 4: reward: -21.000, steps: 764\n",
            "Episode 5: reward: -21.000, steps: 764\n",
            "Episode 6: reward: -21.000, steps: 764\n",
            "Episode 7: reward: -21.000, steps: 764\n",
            "Episode 8: reward: -21.000, steps: 764\n",
            "Episode 9: reward: -21.000, steps: 764\n",
            "Episode 10: reward: -21.000, steps: 764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f37c1e1ac18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}